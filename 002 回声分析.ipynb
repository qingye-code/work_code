{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 拉取订单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn=pymysql.connect(host='mysql-backup.shanzhen.me',\n",
    "                     user='readonly_medicine_liuqq',\n",
    "                     password=\"r3g1^QWgV8GeYJw4\",\n",
    "                     database='asgard_dataplatform',\n",
    "                     port= 3001,\n",
    "                     charset='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = pd.read_excel(r\"C:\\Users\\13670\\Desktop\\dict.xlsx\")\n",
    "id_list = dict.loc[:,'TagID'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_df = pd.DataFrame()\n",
    "\n",
    "for items in id_list:\n",
    "    id_str = \"'\"+ items + \"'\"\n",
    "    tagsql = 'SELECT ORDER_CODE,TAG_ID \\\n",
    "    FROM t66_apollo_offline_analysis_tag \\\n",
    "    WHERE SOURCE_TYPE = 1 \\\n",
    "    AND SZ_STATUS = 1 AND TAG_ID IN ('+id_str+') '\n",
    "    tag = pd.read_sql(tagsql,con=conn)\n",
    "    tag_df = pd.concat([tag_df,tag],axis = 0) \n",
    "    \n",
    "tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_df.to_pickle(r\"C:\\Users\\13670\\Desktop\\fei.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关联分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareData (tagall):\n",
    "    ordercode_list = []\n",
    "    taglist = []\n",
    "    for order_code in np.unique(tagall.ORDER_CODE):\n",
    "        taglist_ = tagall.loc[tagall.ORDER_CODE == order_code, 'TagName'].values.tolist() \n",
    "        ordercode_list.append(order_code)\n",
    "        taglist.append(taglist_)\n",
    "    df = pd.DataFrame({\"order_code\": ordercode_list, \"taglist\":taglist})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AprioriAnalysis(dfData,min_supp,min_conf):\n",
    "    DataList = dfData.taglist.values.tolist()\n",
    "    te = TransactionEncoder()\n",
    "    df_tf = te.fit_transform(DataList)\n",
    "    df = pd.DataFrame(df_tf,columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df,min_support= min_supp,use_colnames= True)\n",
    "    rules = association_rules(frequent_itemsets,metric = 'confidence',min_threshold = min_conf)\n",
    "    # rules = rules.drop(rules[rules.lift <1.0].index)\n",
    "    rules.rename(columns = {'antecedents':'from','consequents':'to','support':'sup','confidence':'conf'},inplace = True)\n",
    "    rules.sort_values(by = ['sup','conf'],ascending = (False,False))\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renal = pd.read_pickle (r'C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\tagall.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renal_dict =  pd.read_excel (r'C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201016\\renal_dict.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renal_ = renal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renal_list_AB = renal_dict.loc[renal_dict.logic1.isin ([\"A\",'a','B','b']),[\"tag_id\",'TagName']]\n",
    "renal_list_A = renal_dict.loc[renal_dict.logic1.isin ([\"A\",'a']),[\"tag_id\",'TagName']]\n",
    "renal_list_B = renal_dict.loc[renal_dict.logic1.isin ([\"B\",'b']),[\"tag_id\",'TagName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renal_AB = pd.merge(renal_,renal_list_AB, left_on = 'TAG_ID', right_on = 'tag_id').drop(columns = ['TAG_ID','tag_id'])\n",
    "renal_A = pd.merge(renal_,renal_list_A, left_on = 'TAG_ID', right_on = 'tag_id').drop(columns = ['TAG_ID','tag_id'])\n",
    "renal_B = pd.merge(renal_,renal_list_B, left_on = 'TAG_ID', right_on = 'tag_id').drop(columns = ['TAG_ID','tag_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AB = PrepareData(renal_AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_AB = AprioriAnalysis(df_AB,0.001,0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 穷举订单"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 描述——结论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagfull = pd.read_pickle(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201202\\tagfull.pkl\")\n",
    "renal = pd.read_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201016\\renal_dict.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desclist = renal.loc[renal['logic']== \"A\",'abbr'].drop_duplicates()\n",
    "summlist = renal.loc[renal['logic']==\"a\",'abbr'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renal_dict = renal.set_index(\"TAG_ID\").to_dict()[\"abbr\"]\n",
    "tagfull.replace(renal_dict,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DescDf = tagfull.loc[tagfull['TAG_ID'].isin(desclist)].drop_duplicates()\n",
    "SummDf = tagfull.loc[tagfull['TAG_ID'].isin(summlist)].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExhaustPatterns(DescDf,SummDf):\n",
    "    ordercode=[]\n",
    "    desclist=[]\n",
    "    summlist=[]\n",
    "    for item in DescDf['ORDER_CODE'].drop_duplicates():\n",
    "        desclistsub=DescDf.loc[DescDf['ORDER_CODE']== item,'TAG_ID'].tolist()\n",
    "        summlistsub=SummDf.loc[SummDf['ORDER_CODE']== item,'TAG_ID'].tolist()\n",
    "        ordercode.append(item)\n",
    "        desclist.append(desclistsub)\n",
    "        summlist.append(summlistsub)\n",
    "    TagOrderCode=pd.DataFrame({'ORDER_CODE':ordercode,'desclist':desclist,'summlist':summlist}) \n",
    "    return TagOrderCode\n",
    "\n",
    "TagOrderCode = ExhaustPatterns(DescDf,SummDf)\n",
    "TagOrderCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TagOrderCode.to_pickle(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201202\\TagOrderCode.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TagOrderCode.applymap(lambda x: tuple(x))\n",
    "df_tuple = pd.DataFrame(df.groupby(['desclist','summlist'])['ORDER_CODE'].count()).reset_index()\n",
    "df_tuple.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201202\\C1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summlist2 = list(summlist)\n",
    "summlist2.insert(-1,\"missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product \n",
    "\n",
    "summ = []\n",
    "desc = []\n",
    "for n in product(desclist,summlist2):\n",
    "    summsub = n[1]\n",
    "    descsub = n[0]\n",
    "    summ.append(summsub)\n",
    "    desc.append(descsub)\n",
    "    \n",
    "solid = pd.DataFrame({'desc':desc,'summ':summ})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solid.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201202\\solid.xlsx\")\n",
    "# 填写solid，命名为solid列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_and_Disassemble(TagOrderCode,solid_path):\n",
    "    df = TagOrderCode.applymap(lambda x: tuple(x))\n",
    "    df_tuple = pd.DataFrame(df.groupby(['desclist','summlist'])['ORDER_CODE'].count()).reset_index()\n",
    "    solid = pd.read_excel(solid_path)\n",
    "    \n",
    "    desc_list = []\n",
    "    summ_list = []\n",
    "    count_list = []\n",
    "    source_model = []\n",
    "    S_NO_desc = []\n",
    "    S_NO_summ = []\n",
    "    S_NO_count = []\n",
    "\n",
    "    for line in df_tuple.values:\n",
    "#         print(line)\n",
    "#         print(line[0])\n",
    "#         print(line[1])\n",
    "        desc = eval(str(line[0]))\n",
    "        summ = eval(str(line[1]))\n",
    "        count = line[2]  \n",
    "\n",
    "        if len(summ)==0:\n",
    "            source = \"missing\"\n",
    "            for index in range(len(desc)):\n",
    "                descsub = desc[index]\n",
    "                desc_list.append(descsub)\n",
    "                summ_list.append(0)\n",
    "                count_list.append(count)\n",
    "                source_model.append(source)\n",
    "\n",
    "        elif (len(desc)== 1 & len(summ)==1):\n",
    "            source = \"complete\"\n",
    "            desc_list.append(desc[0])\n",
    "            summ_list.append(summ[0])\n",
    "            count_list.append(count)\n",
    "            source_model.append(source)\n",
    "        else :\n",
    "            source = \"split\"\n",
    "            descsub_list = []\n",
    "            summsub_list = []\n",
    "\n",
    "            for n in product(desc,summ):\n",
    "                dfsub = pd.DataFrame(product(desc,summ), columns = ['desc','summ'])\n",
    "                dfsub_solid_merge = pd.merge(dfsub,solid,how = 'left')\n",
    "                df_new = dfsub_solid_merge.loc[dfsub_solid_merge.solid.values != 0,[\"desc\",\"summ\"]]\n",
    "            if dfsub_solid_merge.solid.values.sum()== 0 :\n",
    "                    S_NO_desc.append(desc)\n",
    "                    S_NO_summ.append(summ)\n",
    "                    S_NO_count.append(count)                      \n",
    "            for index_n, item in enumerate (np.unique(df_new.desc)):\n",
    "                countsub = count/len(np.unique(df_new.desc))\n",
    "                summsublist = df_new.loc[df_new[\"desc\"].isin ([item]),'summ']\n",
    "                for summsub_ in summsublist:\n",
    "                    countsub_ = countsub/len(summsublist)\n",
    "                    desc_list.append(item)\n",
    "                    summ_list.append(summsub_)\n",
    "                    count_list.append(countsub_)\n",
    "                    source_model.append(source)\n",
    "    final = pd.DataFrame({\"desc\":desc_list,\"summ\":summ_list,\"count\":count_list,\"source_model\":source_model}) \n",
    "    NO_ = pd.DataFrame({\"desc\":S_NO_desc,\"summ\":S_NO_summ,\"count\":S_NO_count})\n",
    "    return final,NO_\n",
    "\n",
    "final,NO = Count_and_Disassemble(TagOrderCode,r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201202\\solid.xlsx\")\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solid = pd.read_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201202\\solid.xlsx\",index_col= False )\n",
    "solid = solid.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = final.copy()\n",
    "final1.loc[final1[\"summ\"] == 0,\"summ\"] =\"missing\"\n",
    "final_ = pd.merge(final1,solid,left_on =[ \"desc\" ,\"summ\"] ,right_on = [\"desc\",\"summ\"])\n",
    "final_.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201217\\final_.xlsx\")\n",
    "NO.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201217\\NO.xlsx\")\n",
    "class_dict = pd.read_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201202\\solid.xlsx\",sheet_name = \"Sheet2\")\n",
    "final_solid = pd.DataFrame(final_.groupby(['desc','summ','source_model',\"solid\"])[\"count\"].sum())\n",
    "final_solid.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201217\\final_solid.xlsx\")\n",
    "final_solid1 = final_solid.copy()\n",
    "final_solid1 = final_solid1.reset_index(['desc','summ','source_model','solid'])\n",
    "class_df = pd.merge(final_solid1,class_dict,left_on = [\"source_model\",\"solid\"],right_on = [\"source_model\",\"solid\"])\n",
    "pic_df = class_df.loc[:,[\"desc\",\"summ\",\"count\",\"result\"]]\n",
    "pic_df_ = pd.DataFrame(pic_df.groupby([\"desc\",\"summ\",\"result\"])[\"count\"].sum())\n",
    "pic_df_ = pic_df_.reset_index(['desc','summ','result'])\n",
    "pic_df_.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201217\\pic.xlsx\")\n",
    "\n",
    "pic_df_1 = pic_df_.loc[pic_df_[\"result\"] != \"NO\",:] # label为NO的订单不计算占比\n",
    "calu = pd.pivot_table(pic_df_1,index = \"desc\",columns = \"summ\",values = \"count\",aggfunc = np.sum)\n",
    "calu[\"sum\"] = calu.sum(axis = 1)\n",
    "picdf1 = pd.DataFrame(pic_df_.groupby([\"desc\",'summ'])[\"count\"].sum()).reset_index([\"desc\",'summ'])\n",
    "sum1 = pd.merge(picdf1,calu,left_on =[ \"desc\"] ,right_on = [\"desc\"])\n",
    "sum2 = sum1.loc[:,['desc','summ','count','sum']]\n",
    "sum2['count_per'] = sum2['count']/sum2['sum']*100\n",
    "sum3 = pd.merge(pic_df_,sum2,left_on =[\"desc\",\"summ\"] ,right_on = [\"desc\",\"summ\"])\n",
    "sum4= sum3.loc[sum3['result'] == \"Maybe\",:]\n",
    "sum4[\"Maybe_per\"] = sum4['count_x']/sum4['count_y']*100\n",
    "sum4 = sum4.loc[:,['desc','summ','Maybe_per']]\n",
    "sum5 = pd.merge(sum2,sum4,left_on =[\"desc\",\"summ\"] ,right_on = [\"desc\",\"summ\"],how = \"outer\")\n",
    "nodf = pic_df_.loc[pic_df_['result'] == \"NO\",['desc','summ','result']]\n",
    "sum6 = pd.merge(sum5,nodf,left_on =[\"desc\",\"summ\"] ,right_on = [\"desc\",\"summ\"],how = \"outer\")\n",
    "sum6.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201219\\summary.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyecharts.charts import Sankey\n",
    "import pyecharts.options as opts\n",
    "\n",
    "picdf = pic_df_.loc[pic_df_.result != \"NO\",['desc','summ','count']]\n",
    "\n",
    "nodes = []\n",
    "\n",
    "for i in range(2):\n",
    "    values = picdf.iloc[:,i].unique()\n",
    "    for value in values:\n",
    "        dict_nodes = {}\n",
    "        dict_nodes[\"name\"] = value\n",
    "        nodes.append(dict_nodes)\n",
    "        \n",
    "links = []\n",
    " \n",
    "for j in picdf.values:\n",
    "    dict_links = {}\n",
    "    dict_links[\"source\"]= j[0]\n",
    "    dict_links[\"target\"]= j[1]\n",
    "    dict_links[\"value\"]= j[2]\n",
    "#     dict_links[\"result\"]= j[3]\n",
    "    links.append(dict_links)\n",
    "    \n",
    "pic = (\n",
    "    Sankey()\n",
    "    .add('回声分析(数量)', \n",
    "         nodes,    \n",
    "         links,\n",
    "         itemstyle_opts=opts.ItemStyleOpts(border_width=4, border_color=\"#aaaa\"),\n",
    "         linestyle_opt=opts.LineStyleOpts(color=\"source\", curve=0.1, opacity=0.4),\n",
    "         tooltip_opts=opts.TooltipOpts(trigger_on=\"mousemove\"),\n",
    "         label_opts=opts.LabelOpts(position=\"right\"),\n",
    "         node_gap = 3,\n",
    "    )\n",
    "    .set_global_opts(title_opts=opts.TitleOpts(title = '肾脏局部病变回声分析'))\n",
    ")\n",
    " \n",
    "pic.render_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结论——描述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExhaustPatterns(DescDf,SummDf):\n",
    "    ordercode=[]\n",
    "    desclist=[]\n",
    "    summlist=[]\n",
    "    for item in SummDf['ORDER_CODE'].drop_duplicates():\n",
    "        desclistsub=DescDf.loc[DescDf['ORDER_CODE']== item,'TAG_ID'].tolist()\n",
    "        summlistsub=SummDf.loc[SummDf['ORDER_CODE']== item,'TAG_ID'].tolist()\n",
    "        ordercode.append(item)\n",
    "        desclist.append(desclistsub)\n",
    "        summlist.append(summlistsub)\n",
    "    TagOrderCode=pd.DataFrame({'ORDER_CODE':ordercode,'summlist':summlist,'desclist':desclist}) \n",
    "    return TagOrderCode\n",
    "\n",
    "TagOrderCode1 = ExhaustPatterns(DescDf,SummDf)\n",
    "TagOrderCode1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = TagOrderCode1.applymap(lambda x: tuple(x))\n",
    "df_tuple = pd.DataFrame(df.groupby(['summlist','desclist'])['ORDER_CODE'].count()).reset_index()\n",
    "df_tuple.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201217\\C1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product \n",
    "\n",
    "desclist2 = list(desclist)\n",
    "desclist2.insert(-1,\"missing\")\n",
    "\n",
    "summ = []\n",
    "desc = []\n",
    "for n in product(desclist2,summlist):\n",
    "    summsub = n[1]\n",
    "    descsub = n[0]\n",
    "    summ.append(summsub)\n",
    "    desc.append(descsub)\n",
    "    \n",
    "solid = pd.DataFrame({'summ':summ,'desc':desc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solid.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201217\\solid.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_and_Disassemble(TagOrderCode,solid_path):\n",
    "    df = TagOrderCode.applymap(lambda x: tuple(x))\n",
    "    df_tuple = pd.DataFrame(df.groupby(['summlist','desclist'])['ORDER_CODE'].count()).reset_index()\n",
    "    solid = pd.read_excel(solid_path)\n",
    "    \n",
    "    desc_list = []\n",
    "    summ_list = []\n",
    "    count_list = []\n",
    "    source_model = []\n",
    "    S_NO_desc = []\n",
    "    S_NO_summ = []\n",
    "    S_NO_count = []\n",
    "\n",
    "    for line in df_tuple.values:\n",
    "#         print(line)\n",
    "#         print(line[0])\n",
    "#         print(line[1])\n",
    "        summ = eval(str(line[1]))\n",
    "        desc = eval(str(line[0]))\n",
    "        count = line[2]  \n",
    "\n",
    "        if len(desc)==0:\n",
    "            source = \"missing\"\n",
    "            for index in range(len(summ)):\n",
    "                summsub = summ[index]\n",
    "                summ_list.append(summsub)\n",
    "                desc_list.append(0)\n",
    "                count_list.append(count)\n",
    "                source_model.append(source)\n",
    "\n",
    "        elif (len(summ)== 1 & len(desc)==1):\n",
    "            source = \"complete\"\n",
    "            desc_list.append(desc[0])\n",
    "            summ_list.append(summ[0])\n",
    "            count_list.append(count)\n",
    "            source_model.append(source)\n",
    "        else :\n",
    "            source = \"split\"\n",
    "            descsub_list = []\n",
    "            summsub_list = []\n",
    "\n",
    "            for n in product(summ,desc):\n",
    "                dfsub = pd.DataFrame(product(summ,desc), columns = ['summ','desc'])\n",
    "                dfsub_solid_merge = pd.merge(dfsub,solid,how = 'left')\n",
    "                df_new = dfsub_solid_merge.loc[dfsub_solid_merge.solid.values != 0,[\"summ\",\"desc\"]]\n",
    "            if dfsub_solid_merge.solid.values.sum()== 0 :\n",
    "                    S_NO_desc.append(desc)\n",
    "                    S_NO_summ.append(summ)\n",
    "                    S_NO_count.append(count)                      \n",
    "            for index_n, item in enumerate (np.unique(df_new.summ)):\n",
    "                countsub = count/len(np.unique(df_new.summ))\n",
    "                descsublist = df_new.loc[df_new[\"summ\"].isin ([item]),'desc']\n",
    "                for descsub_ in descsublist:\n",
    "                    countsub_ = countsub/len(descsublist)\n",
    "                    summ_list.append(item)\n",
    "                    desc_list.append(descsub_)\n",
    "                    count_list.append(countsub_)\n",
    "                    source_model.append(source)\n",
    "    final = pd.DataFrame({\"summ\":summ_list,\"desc\":desc_list,\"count\":count_list,\"source_model\":source_model}) \n",
    "    NO_ = pd.DataFrame({\"summ\":S_NO_summ,\"desc\":S_NO_desc,\"count\":S_NO_count})\n",
    "    return final,NO_\n",
    "\n",
    "final,NO = Count_and_Disassemble(TagOrderCode1,r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201217\\solid.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = final.copy()\n",
    "final1.loc[final1[\"desc\"] == 0,\"desc\"] =\"missing\"\n",
    "final_ = pd.merge(final1,solid,left_on =[ \"desc\" ,\"summ\"] ,right_on = [\"desc\",\"summ\"])\n",
    "final_.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201218\\final_.xlsx\")\n",
    "NO.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201218\\NO.xlsx\")\n",
    "class_dict = pd.read_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201202\\solid.xlsx\",sheet_name = \"Sheet2\")\n",
    "final_solid = pd.DataFrame(final_.groupby(['summ','desc','source_model',\"solid\"])[\"count\"].sum())\n",
    "final_solid.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201218\\final_solid.xlsx\")\n",
    "final_solid1 = final_solid.copy()\n",
    "final_solid1 = final_solid1.reset_index(['summ','desc','source_model','solid'])\n",
    "class_df = pd.merge(final_solid1,class_dict,left_on = [\"source_model\",\"solid\"],right_on = [\"source_model\",\"solid\"])\n",
    "pic_df = class_df.loc[:,[\"desc\",\"summ\",\"count\",\"result\"]]\n",
    "pic_df_ = pd.DataFrame(pic_df.groupby([\"summ\",\"desc\",\"result\"])[\"count\"].sum())\n",
    "pic_df_ = pic_df_.reset_index(['summ','desc','result'])\n",
    "pic_df_.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201218\\pic.xlsx\")\n",
    "\n",
    "pic_df_1 = pic_df_.loc[pic_df_[\"result\"] != \"NO\",:]\n",
    "calu = pd.pivot_table(pic_df_1,index = \"summ\",columns = \"desc\",values = \"count\",aggfunc = np.sum)\n",
    "calu[\"sum\"] = calu.sum(axis = 1)\n",
    "picdf1 = pd.DataFrame(pic_df_.groupby(['summ',\"desc\"])[\"count\"].sum()).reset_index(['summ',\"desc\"])\n",
    "sum1 = pd.merge(picdf1,calu,left_on =[\"summ\"] ,right_on = [\"summ\"])\n",
    "sum2 = sum1.loc[:,['desc','summ','count','sum']]\n",
    "sum2['count_per'] = sum2['count']/sum2['sum']*100\n",
    "sum3 = pd.merge(pic_df_,sum2,left_on =[\"desc\",\"summ\"] ,right_on = [\"desc\",\"summ\"])\n",
    "sum4= sum3.loc[sum3['result'] == \"Maybe\",:]\n",
    "sum4[\"Maybe_per\"] = sum4['count_x']/sum4['count_y']*100\n",
    "sum4 = sum4.loc[:,['desc','summ','Maybe_per']]\n",
    "sum5 = pd.merge(sum2,sum4,left_on =[\"desc\",\"summ\"] ,right_on = [\"desc\",\"summ\"],how = \"outer\")\n",
    "nodf = pic_df_.loc[pic_df_['result'] == \"NO\",['desc','summ','result']]\n",
    "sum6 = pd.merge(sum5,nodf,left_on =[\"desc\",\"summ\"] ,right_on = [\"desc\",\"summ\"],how = \"outer\")\n",
    "sum6.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201218\\summary.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵数据 & 热力图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data为将两次结果中的名称放到一起，作为矩阵的边\n",
    "data = pd.read_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201219\\test.xlsx\",sheet_name = \"Sheet2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import itertools \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = pd.DataFrame(index=[i for i in np.unique(data.desc)],\\\n",
    "        columns=[i for i in np.unique(data.summ)])\n",
    "\n",
    "for desc,summ in itertools.product(np.unique(data.desc),\\\n",
    "                               np.unique(data.summ)):\n",
    "    data1 = data.loc[data.desc== desc,:]\n",
    "    if desc == summ:\n",
    "        perc = [0]\n",
    "        results.loc[desc, summ] = 0\n",
    "    else:\n",
    "        perc = data1.loc[data1.summ== summ,\"count_per\"].values\n",
    "        if len(perc):\n",
    "            results.loc[desc, summ] = round(perc[0],2)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_excel(r\"C:\\Users\\13670\\Desktop\\待完成\\项目4 回声tag分析\\肾脏\\20201219\\matrix1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2_ = results[results.columns].astype(float)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "ax = sns.heatmap(results2_,\n",
    "                 annot=True,\n",
    "             annot_kws={'size':12,'weight':'bold', 'color':'black'},\n",
    "                 vmax=100, \n",
    "                 square=True, \n",
    "                 cmap=\"Blues\",\n",
    "                 center= 20\n",
    "                )\n",
    "\n",
    "ax.set_title('Renal_Ultra_Sound')\n",
    "ax.set_xlabel('TO')\n",
    "ax.set_ylabel(\"FROM\")\n",
    "# plt.rcParams['font.sans-serif']=['FangSong']\n",
    "plt.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = results.fillna(float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_Matrix:\n",
    "    \"\"\"\n",
    "    Adjacency Matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, vertices=[], matrix=[]):\n",
    "        \"\"\"\n",
    "        :param vertices:a dict with vertex id and index of matrix , such as {vertex:index}\n",
    "        :param matrix: a matrix\n",
    "        \"\"\"\n",
    "        self.matrix = matrix\n",
    "        self.edges_dict = {}  # {(tail, head):weight}\n",
    "        self.edges_array = []  # (tail, head, weight)\n",
    "        self.vertices = vertices\n",
    "        self.num_edges = 0\n",
    "\n",
    "        # if provide adjacency matrix then create the edges list\n",
    "        if len(matrix) > 0:\n",
    "            if len(vertices) != len(matrix):\n",
    "                raise IndexError\n",
    "            self.edges = self.getAllEdges()\n",
    "            self.num_edges = len(self.edges)\n",
    "\n",
    "        # if do not provide a adjacency matrix, but provide the vertices list, build a matrix with 0\n",
    "        elif len(vertices) > 0:\n",
    "            self.matrix = [[0 for col in range(len(vertices))] for row in range(len(vertices))]\n",
    "\n",
    "        self.num_vertices = len(self.matrix)\n",
    "\n",
    "    def isOutRange(self, x):\n",
    "        try:\n",
    "            if x >= self.num_vertices or x <= 0:\n",
    "                raise IndexError\n",
    "        except IndexError:\n",
    "            print(\"节点下标出界\")\n",
    "\n",
    "    def isEmpty(self):\n",
    "        if self.num_vertices == 0:\n",
    "            self.num_vertices = len(self.matrix)\n",
    "        return self.num_vertices == 0\n",
    "\n",
    "    def add_vertex(self, key):\n",
    "        if key not in self.vertices:\n",
    "            self.vertices[key] = len(self.vertices) + 1\n",
    "\n",
    "        # add a vertex mean add a row and a column\n",
    "        # add a column for every row\n",
    "        for i in range(self.getVerticesNumbers()):\n",
    "            self.matrix[i].append(0)\n",
    "\n",
    "        self.num_vertices += 1\n",
    "\n",
    "        nRow = [0] * self.num_vertices\n",
    "        self.matrix.append(nRow)\n",
    "\n",
    "    def getVertex(self, key):\n",
    "        pass\n",
    "\n",
    "    def add_edges_from_list(self, edges_list):  # edges_list : [(tail, head, weight),()]\n",
    "        for i in range(len(edges_list)):\n",
    "            self.add_edge(edges_list[i][0], edges_list[i][1], edges_list[i][2], )\n",
    "\n",
    "    def add_edge(self, tail, head, cost=0):\n",
    "        # if self.vertices.index(tail) >= 0:\n",
    "        #   self.addVertex(tail)\n",
    "        if tail not in self.vertices:\n",
    "            self.add_vertex(tail)\n",
    "        # if self.vertices.index(head) >= 0:\n",
    "        #   self.addVertex(head)\n",
    "        if head not in self.vertices:\n",
    "            self.add_vertex(head)\n",
    "\n",
    "        # for directory matrix\n",
    "        self.matrix[self.vertices.index(tail)][self.vertices.index(head)] = cost\n",
    "        # for non-directory matrix\n",
    "        # self.matrix[self.vertices.index(fromV)][self.vertices.index(toV)] = \\\n",
    "        #   self.matrix[self.vertices.index(toV)][self.vertices.index(fromV)] = cost\n",
    "\n",
    "        self.edges_dict[(tail, head)] = cost\n",
    "        self.edges_array.append((tail, head, cost))\n",
    "        self.num_edges = len(self.edges_dict)\n",
    "\n",
    "    def getEdges(self, V):\n",
    "        pass\n",
    "\n",
    "    def getVerticesNumbers(self):\n",
    "        if self.num_vertices == 0:\n",
    "            self.num_vertices = len(self.matrix)\n",
    "        return self.num_vertices\n",
    "\n",
    "    def getAllVertices(self):\n",
    "        return self.vertices\n",
    "\n",
    "    def getAllEdges(self):\n",
    "        for i in range(len(self.matrix)):\n",
    "            for j in range(len(self.matrix)):\n",
    "                if 0 < self.matrix[i][j] < float('inf'):\n",
    "                    self.edges_dict[self.vertices[i], self.vertices[j]] = self.matrix[i][j]\n",
    "                    self.edges_array.append([self.vertices[i], self.vertices[j], self.matrix[i][j]])\n",
    "\n",
    "        return self.edges_array\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(''.join(str(i) for i in self.matrix))\n",
    "\n",
    "    def to_do_vertex(self, i):\n",
    "        print('vertex: %s' % (self.vertices[i]))\n",
    "\n",
    "    def to_do_edge(self, w, k):\n",
    "        print('edge tail: %s, edge head: %s, weight: %s' % (self.vertices[w], self.vertices[k], str(self.matrix[w][k])))\n",
    "        \n",
    "my_graph = Graph_Matrix()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_directed_graph(my_graph):\n",
    "    G = nx.DiGraph()  # 建立一个空的无向图G\n",
    "    for node in my_graph.vertices:\n",
    "        G.add_node(str(node))\n",
    "    G.add_weighted_edges_from(my_graph.edges_array)\n",
    "\n",
    "    print(\"nodes:\", G.nodes())  \n",
    "    print(\"edges:\", G.edges()) \n",
    "    print(\"number of edges:\", G.number_of_edges())  \n",
    "    \n",
    "    plt.subplots(figsize=(8, 6))   \n",
    "    nx.draw(G, with_labels=True,pos=nx.spring_layout(G),node_size = 500,\n",
    "        width=[float(v['weight'])/100  for (r, c, v) in G.edges(data=True)])\n",
    "    \n",
    "    plt.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "    plt.rcParams['axes.unicode_minus']=False\n",
    "    plt.savefig(\"directed_graph.png\")\n",
    "    plt.show()\n",
    "    return G\n",
    "    \n",
    "def create_directed_matrix(my_graph,data):\n",
    "    data1 = data.fillna(float('inf'))\n",
    "    nodes = list(data.columns)\n",
    "    matrix = data1.values\n",
    "    my_graph = Graph_Matrix(nodes, matrix)\n",
    "    return my_graph\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    my_graph = Graph_Matrix()\n",
    "    created_graph = create_directed_matrix(my_graph,results1_)\n",
    "    G = draw_directed_graph(created_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成邻近矩阵\n",
    "a= nx.adjacency_matrix(G)\n",
    "A=a.todense()\n",
    "\n",
    "import numpy\n",
    "numpy.savetxt('correlation_matrix.csv',A, delimiter = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_directed_graph(my_graph):\n",
    "    G = nx.DiGraph()  # 建立一个空的无向图G\n",
    "    for node in my_graph.vertices:\n",
    "        G.add_node(str(node))\n",
    "    G.add_weighted_edges_from(my_graph.edges_array) \n",
    "    \n",
    "    plt.subplots(figsize=(8, 6)) \n",
    "    \n",
    "    elarge=[(u,v) for (u,v,d) in G.edges(data=True) if d['weight']>20]\n",
    "    esmall=[(u,v) for (u,v,d) in G.edges(data=True) if (d['weight']>0)&(d['weight']<=20)]\n",
    "    nx.circular_layout(G)##图的布局方式，圆形\n",
    "    pos=nx.spring_layout(G)\n",
    "    nx.draw_networkx_nodes(G,pos,alpha=0.4,node_size = 600)\n",
    "    #设置边的形式\n",
    "    nx.draw_networkx_edges(G,pos,edgelist=elarge,width=3,alpha=1,edge_color='r')\n",
    "    nx.draw_networkx_edges(G,pos,edgelist=esmall,width=1,alpha=0.8,edge_color='b',style='dashed')\n",
    "    nx.draw_networkx_labels(G,pos,font_size=10)\n",
    "    \n",
    "    plt.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "    plt.rcParams['axes.unicode_minus']=False\n",
    "    plt.savefig(\"directed_graph.png\")\n",
    "    plt.show()\n",
    "    return G\n",
    "    \n",
    "def create_directed_matrix(my_graph,data):\n",
    "    data1 = data.fillna(float('inf'))\n",
    "    nodes = list(data.columns)\n",
    "    matrix = data1.values\n",
    "    my_graph = Graph_Matrix(nodes, matrix)\n",
    "    return my_graph\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    my_graph = Graph_Matrix()\n",
    "    created_graph = create_directed_matrix(my_graph,results1_)\n",
    "    G = draw_directed_graph(created_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取异常订单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recode_df = pd.DataFrame()\n",
    "\n",
    "for items in tag_df1.ORDER_CODE:\n",
    "    ORDER_CODE_str = \"'\"+ items + \"'\"\n",
    "    \n",
    "    sqlstr = \"SELECT DISTINCT RR.ORDER_CODE,RR2.RAW_NAME,RR.RAW_NAME,RR.RAW_RESULT AS '三级项结果',RR2.RAW_RESULT AS '小结',DR.SD_CODE \\\n",
    "    FROM T66_ATHENA_BASIC_RAW_RECORD AS RR LEFT JOIN T66_ATHENA_BASIC_DESCRIPTIVE_RECORD AS DR ON RR.ORDER_CODE = DR.ORDER_CODE \\\n",
    "    AND RR.RECORD_CODE = DR.RECORD_CODE AND DR.SD_CODE IN ('S0000005D','S0000006D','S0000007D') \\\n",
    "    AND RR.ORDER_CODE IN (\" + ORDER_CODE_str + \")AND DR.SZ_STATUS = 1 \\\n",
    "    LEFT JOIN T66_ATHENA_BASIC_RAW_RECORD AS RR2 ON RR.ORDER_CODE = RR2.ORDER_CODE \\\n",
    "    AND RR.PARENT_RECORD_CODE = RR2.RECORD_CODE \\\n",
    "    WHERE \\\n",
    "    DR.ID IS NOT NULL \\\n",
    "    AND RR.SZ_STATUS = 1 \\\n",
    "    AND DR.SZ_STATUS = 1 \\\n",
    "    AND RR2.SZ_STATUS = 1 \" \n",
    "\n",
    "    Recode = pd.read_sql(sqlstr,con=conn)\n",
    "    Recode_df = pd.concat([Recode_df,Recode],axis = 0) \n",
    "\n",
    "Recode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
