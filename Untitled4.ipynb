{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T07:01:03.619644Z",
     "start_time": "2020-12-15T07:00:54.807020Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置日志的级别，共分为debug info warning error critical\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "# transformers是tf和pytorch最新的自然语言处理库，提供自然语言理解（NLU）和自然语言生成（NLG）的最先进的模型（Bert、GPT-2···）\n",
    "# 拥有超过32种预训练模型，支持100多种语言，在tensorflow2.0和pytorch之间具有深厚的互操作性\n",
    "from transformers import TFBertForSequenceClassification,BertTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T07:02:51.277977Z",
     "start_time": "2020-12-15T07:02:46.436778Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T07:02:55.295781Z",
     "start_time": "2020-12-15T07:02:55.202876Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T07:05:45.454059Z",
     "start_time": "2020-12-15T07:05:45.444009Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review): \n",
    "  # combine step for tokenization, WordPiece vector mapping, adding special tokens as well as truncating reviews longer than the max length\n",
    "    return tokenizer.encode_plus(review, \n",
    "            add_special_tokens = True, # add [CLS], [SEP]\n",
    "            max_length = max_length, # max length of the text that can go to BERT\n",
    "            pad_to_max_length = True, # add [PAD] tokens\n",
    "            return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "    truncation=True\n",
    "          )\n",
    "# map to the expected input to TFBertForSequenceClassification, see here \n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "def encode_examples(ds, limit=-1):\n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "  \n",
    "    for index, row in ds.iterrows():\n",
    "        review = row[\"text\"]\n",
    "        label = row[\"y\"]\n",
    "        bert_input = convert_example_to_feature(review)\n",
    "  \n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T07:11:08.278487Z",
     "start_time": "2020-12-15T07:11:08.273466Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(df):\n",
    "    train_set, x = train_test_split(df, \n",
    "        stratify=df['label'],\n",
    "        test_size=0.1, \n",
    "        random_state=42)\n",
    "    val_set, test_set = train_test_split(x, \n",
    "        stratify=x['label'],\n",
    "        test_size=0.5, \n",
    "        random_state=43)\n",
    "\n",
    "    return train_set,val_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-15T07:11:54.822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59e3376593e47e1b954ce3559099e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=109540.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13670\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a65dc36a3341f28aab7d76728ce581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=624.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25c930319df429c8a8f5432ac39f69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=478309336.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier', 'dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1407/1407 [==============================] - 32178s 23s/step - loss: 0.3012 - accuracy: 0.9105 - val_loss: 0.2074 - val_accuracy: 0.9325\n",
      "Epoch 2/8\n",
      "1407/1407 [==============================] - 29401s 21s/step - loss: 0.1607 - accuracy: 0.9473 - val_loss: 0.1903 - val_accuracy: 0.9399\n",
      "Epoch 3/8\n",
      "1407/1407 [==============================] - 30103s 21s/step - loss: 0.1135 - accuracy: 0.9619 - val_loss: 0.2052 - val_accuracy: 0.9374\n",
      "Epoch 4/8\n",
      " 762/1407 [===============>..............] - ETA: 3:59:57 - loss: 0.0848 - accuracy: 0.9714"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "\n",
    "    # parameters\n",
    "    data_path = (r\"C:\\Users\\13670\\Desktop\\test\\NLP_bert\\data.txt\") # 数据路径\n",
    "    model_path = \"bert-base-chinese\" #模型路径，建议预先下载(https://huggingface.co/bert-base-chinese#)\n",
    "\n",
    "    max_length = 32\n",
    "    batch_size = 128\n",
    "    learning_rate = 2e-5\n",
    "    number_of_epochs = 8\n",
    "    num_classes = 10 # 类别数\n",
    "\n",
    "    # read data\n",
    "    df_raw = pd.read_csv(data_path,sep=\"\\t\",header=None,names=[\"text\",\"label\"])    \n",
    "    # transfer label\n",
    "    df_label = pd.DataFrame({\"label\":[\"财经\",\"房产\",\"股票\",\"教育\",\"科技\",\"社会\",\"时政\",\"体育\",\"游戏\",\"娱乐\"],\"y\":list(range(10))})\n",
    "    df_raw = pd.merge(df_raw,df_label,on=\"label\",how=\"left\")\n",
    "    # split data\n",
    "    train_data,val_data, test_data = split_dataset(df_raw)\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    # train dataset\n",
    "    ds_train_encoded = encode_examples(train_data).shuffle(10000).batch(batch_size)\n",
    "    # val dataset\n",
    "    ds_val_encoded = encode_examples(val_data).batch(batch_size)\n",
    "    # test dataset\n",
    "    ds_test_encoded = encode_examples(test_data).batch(batch_size)\n",
    "\n",
    "    # model initialization\n",
    "    # model = TFBertForMultilabelClassification.from_pretrained(model_path, num_labels=num_classes)\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=num_classes)\n",
    "\n",
    "    # optimizer Adam recommended\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,epsilon=1e-08, clipnorm=1)\n",
    "    # we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "    # fit model\n",
    "    bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_val_encoded)\n",
    "    # evaluate test_set\n",
    "    print(\"# evaluate test_set:\",model.evaluate(ds_test_encoded))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
